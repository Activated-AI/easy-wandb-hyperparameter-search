# Easy WandB Hyperparameter Search

This repository demonstrates how to use Weights & Biases (WandB) for hyperparameter optimization in a PyTorch-based machine learning project. It includes a simple MNIST classifier and utilizes WandB's sweep functionality to find optimal hyperparameters. Additionally, it supports running hyperparameter sweeps on Lightning AI.

## Project Structure

- `worker.py`: Runs the WandB agent for hyperparameter search.
- `train.py`: Contains the main training loop and model definition.
- `sweep.py`: Sets up and initiates the WandB sweep.
- `sweeplightning.py`: Sets up and runs WandB sweep using Lightning AI.
- `train.ipynb`: Jupyter notebook version of the training script.

## Usage

1. Start a hyperparameter sweep:
   ```
   python sweep.py
   ```

2. Add an additional worker to participate in a running sweep (`sweep.py` automatically starts a worker on the machine it is run on):
   ```
   python worker.py -s <SWEEP_ID> -p <PROJECT_NAME>
   ```

   Replace `<SWEEP_ID>` with the ID generated by `sweep.py` and `<PROJECT_NAME>` with your WandB project name.

3. To run a sweep using Lightning AI:
   ```
   python sweeplightning.py
   ```

   This will set up the sweep and run multiple workers on Lightning AI's infrastructure. By default, it creates 3 worker jobs, each running on a separate A10G machine. You can adjust the number of workers by modifying the `num_workers` variable in the `main()` function of `sweeplightning.py`.

## Lightning AI Configuration

The `sweeplightning.py` script uses Lightning AI's Studio and Jobs plugin to distribute the hyperparameter search across multiple machines:

- It creates a Studio instance named "hyperparameter search".
- It uses the Jobs plugin to spawn multiple worker jobs.
- Each worker runs on an A10G machine (configurable via the `Machine` enum in the script).
- The number of concurrent workers is set to 3 by default but can be easily adjusted.

To modify the number of machines or machine type:

1. Change the `num_workers` variable in the `main()` function to adjust the number of concurrent jobs.
2. Modify the `Machine` enum value in the `jobs_plugin.run()` call to change the machine type for each worker.

## Customization

- Modify `sweep.py` or `sweeplightning.py` to adjust the hyperparameter search space.
- Edit `train.py` or `train.ipynb` to change the model architecture or training process.

## Features

- Supports both script (`train.py`) and notebook (`train.ipynb`) training methods.
- Utilizes WandB for experiment tracking and hyperparameter optimization.
- Implements a simple CNN for MNIST classification.
- Supports running distributed hyperparameter sweeps on Lightning AI infrastructure.
- Configurable number of concurrent workers and machine types on Lightning AI.

## Contributing

Feel free to fork this repository and submit pull requests with improvements or additional features.

## License

[MIT License](https://opensource.org/licenses/MIT)