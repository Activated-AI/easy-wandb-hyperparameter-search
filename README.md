# Easy WandB Hyperparameter Search

This repository demonstrates how to use Weights & Biases (WandB) for hyperparameter optimization in a PyTorch-based machine learning project. It includes a simple MNIST classifier and utilizes WandB's sweep functionality to find optimal hyperparameters. Additionally, it supports running hyperparameter sweeps on Lightning AI.

## Project Structure

- `worker.py`: Runs the WandB agent for hyperparameter search.
- `train.py`: Contains the main training loop and model definition.
- `sweep.py`: Sets up and initiates the WandB sweep.
- `sweeplightning.py`: Sets up and runs WandB sweep using Lightning AI.
- `train.ipynb`: Jupyter notebook version of the training script.

## Usage

1. Start a hyperparameter sweep:
   ```
   python sweep.py
   ```

2. Run a worker to participate in the sweep:
   ```
   python worker.py -s <SWEEP_ID> -p <PROJECT_NAME>
   ```

   Replace `<SWEEP_ID>` with the ID generated by `sweep.py` and `<PROJECT_NAME>` with your WandB project name.

3. To run a sweep using Lightning AI:
   ```
   python sweeplightning.py
   ```

   This will set up the sweep and run multiple workers on Lightning AI's infrastructure.

### Lightning AI Workers

The `sweeplightning.py` script is configured to run multiple workers in parallel on Lightning AI's infrastructure. By default, it's set to run 3 workers, each on an A10G machine. You can adjust the number of workers by modifying the `num_workers` variable in the `main()` function of `sweeplightning.py`:

```python
def main():
    num_workers = 3  # Adjust this value to change the number of parallel workers
    sweep_id, projectname = setup_wandb_sweep()
    run_workers(sweep_id, projectname, num_workers)
```

Increasing the number of workers will allow for more parallel exploration of the hyperparameter space, potentially finding optimal configurations faster. However, keep in mind that this will also increase resource usage on the Lightning AI platform.

Each worker runs independently, executing the `worker.py` script with the appropriate sweep ID and project name. The Lightning AI Studio manages these workers, distributing them across the specified machine type (A10G in this case).

## Customization

- Modify `sweep.py` or `sweeplightning.py` to adjust the hyperparameter search space.
- Edit `train.py` or `train.ipynb` to change the model architecture or training process.

## Features

- Supports both script (`train.py`) and notebook (`train.ipynb`) training methods.
- Utilizes WandB for experiment tracking and hyperparameter optimization.
- Implements a simple CNN for MNIST classification.
- Supports running hyperparameter sweeps on Lightning AI infrastructure with configurable number of parallel workers.

## Contributing

Feel free to fork this repository and submit pull requests with improvements or additional features.

## License

[MIT License](https://opensource.org/licenses/MIT)